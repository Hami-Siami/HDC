{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "$$HDC\\ voice\\ Baseline\\ : ISOLET\\ (voiceHD-style)$$"
      ],
      "metadata": {
        "id": "Rg4U3SToyI3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "AmWkbpUSycnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np, time, os\n",
        "\n",
        "def set_seed(seed=123):\n",
        "    import random\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "set_seed(123)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vHoMzEaydaw",
        "outputId": "3320077f-7f25-42c4-b74b-6cbf828e348e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We quantize per feature (train-only min/max), then encode with iM âŠ™ CiM and bundle.\n",
        "\n"
      ],
      "metadata": {
        "id": "QfwVTCLkyilN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "I6y0v1HqyobH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch ISOLET"
      ],
      "metadata": {
        "id": "8jXc2li8y4gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We fetch ISOLET from OpenML.\n",
        "iso = fetch_openml('isolet', version=1, as_frame=False)\n",
        "X = iso['data'].astype(np.float32)  # (N, 617)\n",
        "y = iso['target']\n",
        "\n",
        "# Map labels 'A'..'Z' to 0..25\n",
        "classes = sorted(np.unique(y).tolist())\n",
        "label_to_id = {c:i for i,c in enumerate(classes)}\n",
        "y_int = np.array([label_to_id[s] for s in y], dtype=np.int64)\n",
        "\n",
        "# Simple stratified split: 80% train, 20% test\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X, y_int, test_size=0.20, random_state=123, stratify=y_int\n",
        ")\n",
        "\n",
        "N, F = X.shape\n",
        "C = len(classes)\n",
        "N, F, C\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3gLgF9ayo2F",
        "outputId": "6020ed1a-6ed0-4271-e0e6-8c807c7e57f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7797, 617, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization"
      ],
      "metadata": {
        "id": "i8yNiacUyyM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEVELS = 21  # We use 21 levels (0..20) as a classic VoiceHD choice.\n",
        "\n",
        "Xmin = X_tr.min(axis=0)\n",
        "Xmax = X_tr.max(axis=0)\n",
        "rng  = np.maximum(Xmax - Xmin, 1e-8)\n",
        "\n",
        "def quantize_to_levels(Xf: np.ndarray, levels: int = LEVELS) -> np.ndarray:\n",
        "    Z = (Xf - Xmin) / rng\n",
        "    L = np.clip(np.round(Z * (levels - 1)), 0, levels - 1).astype(np.int64)\n",
        "    return L\n",
        "\n",
        "L_tr = quantize_to_levels(X_tr, LEVELS)\n",
        "L_te = quantize_to_levels(X_te, LEVELS)\n",
        "\n",
        "L_tr_t, y_tr_t = torch.from_numpy(L_tr), torch.from_numpy(y_tr)\n",
        "L_te_t, y_te_t = torch.from_numpy(L_te), torch.from_numpy(y_te)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "train_loader = DataLoader(TensorDataset(L_tr_t, y_tr_t), batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=False)\n",
        "test_loader  = DataLoader(TensorDataset(L_te_t, y_te_t), batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=False)\n"
      ],
      "metadata": {
        "id": "RY1ghFb0yzv0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "-LUCwXaGzMoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def bipolar_sign(x: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.where(x >= 0, torch.ones_like(x), -torch.ones_like(x))\n",
        "\n",
        "def _rand_bip(shape, device=None):\n",
        "    device = device or DEVICE\n",
        "    r = torch.randint(0, 2, shape, device=device, dtype=torch.int8)\n",
        "    return r.float().mul_(2).sub_(1)\n",
        "\n",
        "def make_item_memory(F: int, D: int, device=None) -> torch.Tensor:\n",
        "    # I assign each feature a random bipolar HV (iM).\n",
        "    return _rand_bip((F, D), device=device or DEVICE)\n",
        "\n",
        "def make_cim(levels: int, D: int, device=None) -> torch.Tensor:\n",
        "    # I build CiM with progressive bit flips so adjacent levels are similar.\n",
        "    device = device or DEVICE\n",
        "    base = _rand_bip((D,), device=device)\n",
        "    perm = torch.randperm(D, device=device)\n",
        "    out  = torch.empty((levels, D), device=device, dtype=torch.float32)\n",
        "    for l in range(levels):\n",
        "        k  = (l * D) // max(1, levels - 1)\n",
        "        hv = base.clone()\n",
        "        if k > 0:\n",
        "            hv[perm[:k]] = -hv[perm[:k]]\n",
        "        out[l] = hv\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_isolet_batch(L_batch: torch.Tensor,  # [B, F] ints in [0..L-1]\n",
        "                        iM: torch.Tensor,       # [F, D]\n",
        "                        CiM: torch.Tensor,      # [L, D]\n",
        "                        block_features: int = 64) -> torch.Tensor:\n",
        "    # I encode each sample by binding iM[f] with CiM[level[f]] and bundling over features.\n",
        "    B, F = L_batch.shape\n",
        "    D = iM.shape[1]\n",
        "    hv_sum = torch.zeros((B, D), device=iM.device, dtype=torch.float32)\n",
        "    for f0 in range(0, F, block_features):\n",
        "        f1 = min(f0 + block_features, F)\n",
        "        CiM_sel = CiM[L_batch[:, f0:f1].long().to(iM.device)]  # [B, block, D]\n",
        "        iM_blk  = iM[f0:f1].unsqueeze(0)                       # [1, block, D]\n",
        "        hv_sum.add_((CiM_sel * iM_blk).sum(dim=1))\n",
        "    return bipolar_sign(hv_sum)\n",
        "\n",
        "@torch.no_grad()\n",
        "def build_class_prototypes(loader: DataLoader,\n",
        "                           iM: torch.Tensor,\n",
        "                           CiM: torch.Tensor,\n",
        "                           n_classes: int,\n",
        "                           block_features: int = 64) -> torch.Tensor:\n",
        "    D = iM.shape[1]\n",
        "    accum = torch.zeros((n_classes, D), device=iM.device, dtype=torch.float32)\n",
        "    for Lb, yb in loader:\n",
        "        Lb = Lb.to(iM.device, non_blocking=True)\n",
        "        yb = yb.to(iM.device, non_blocking=True)\n",
        "        hvs = encode_isolet_batch(Lb, iM, CiM, block_features=block_features)\n",
        "        for c in range(n_classes):\n",
        "            m = (yb == c)\n",
        "            if m.any():\n",
        "                accum[c] += hvs[m].sum(dim=0)\n",
        "    return bipolar_sign(accum)\n",
        "\n",
        "@torch.no_grad()\n",
        "def cosine_sim(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "    a32 = a.float(); b32 = b.float()\n",
        "    an = torch.linalg.norm(a32, dim=1, keepdim=True).clamp_min_(1e-8)\n",
        "    bn = torch.linalg.norm(b32, dim=1, keepdim=True).clamp_min_(1e-8).T\n",
        "    return (a32 @ b32.T) / (an * bn)\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_with_prototypes(L_batch: torch.Tensor,\n",
        "                            iM: torch.Tensor,\n",
        "                            CiM: torch.Tensor,\n",
        "                            prototypes: torch.Tensor,\n",
        "                            block_features: int = 64) -> torch.Tensor:\n",
        "    hvs  = encode_isolet_batch(L_batch, iM, CiM, block_features=block_features)\n",
        "    sims = cosine_sim(hvs, prototypes)\n",
        "    return sims.argmax(dim=1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_loader(loader: DataLoader,\n",
        "                    iM: torch.Tensor,\n",
        "                    CiM: torch.Tensor,\n",
        "                    prototypes: torch.Tensor,\n",
        "                    block_features: int = 64) -> float:\n",
        "    correct = total = 0\n",
        "    for Lb, yb in loader:\n",
        "        Lb = Lb.to(iM.device, non_blocking=True)\n",
        "        yb = yb.to(iM.device, non_blocking=True)\n",
        "        preds = predict_with_prototypes(Lb, iM, CiM, prototypes, block_features=block_features)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total   += yb.numel()\n",
        "    return 100.0 * correct / total\n"
      ],
      "metadata": {
        "id": "TJf2eOx7zDiE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIM = 10_000  # We typically use 8kâ€“20k; 10k is a solid default.\n",
        "iM  = make_item_memory(F, DIM, device=DEVICE)\n",
        "CiM = make_cim(LEVELS, DIM, device=DEVICE)\n",
        "\n",
        "t0 = time.time()\n",
        "prototypes = build_class_prototypes(train_loader, iM, CiM, n_classes=C, block_features=64)\n",
        "print(\"Prototypes:\", prototypes.shape, \"built in %.2fs\" % (time.time() - t0))\n",
        "\n",
        "acc = evaluate_loader(test_loader, iM, CiM, prototypes, block_features=64)\n",
        "print(f\"Test accuracy (ISOLET; VoiceHD-style): {acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "iLF0vjlQzOtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "_FM1m199wi2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize HDC space"
      ],
      "metadata": {
        "id": "z1WkH4_nwnf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIM = 10_000  # We typically use 8kâ€“20k; 10k is a solid default.\n",
        "iM  = make_item_memory(F, DIM, device=DEVICE)\n",
        "CiM = make_cim(LEVELS, DIM, device=DEVICE)"
      ],
      "metadata": {
        "id": "qDM3OmWYwm5M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train prototypes"
      ],
      "metadata": {
        "id": "hgQP3eIWwkpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t0 = time.time()\n",
        "prototypes = build_class_prototypes(train_loader, iM, CiM, n_classes=C, block_features=64)\n",
        "print(\"Prototypes:\", prototypes.shape, \"built in %.2fs\" % (time.time() - t0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89GTfYBXwjhu",
        "outputId": "cfe0ef79-6a68-4aa0-c8bb-83af488dbfa6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prototypes: torch.Size([26, 10000]) built in 250.16s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(prototypes, \"prototypes_ISOLET.pt\")"
      ],
      "metadata": {
        "id": "aOoCn40bxLIu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "c_Gc3C6iw4kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.load(\"prototypes_ISOLET.pt\")"
      ],
      "metadata": {
        "id": "tfaFfHFKxVsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = evaluate_loader(test_loader, iM, CiM, prototypes, block_features=64)\n",
        "print(f\"Test accuracy (ISOLET; VoiceHD-style): {acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IcdJx-MwvJF",
        "outputId": "caeac25a-239d-4fed-c74d-eecb56c55542"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy (ISOLET; VoiceHD-style): 87.95%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zGqZCqDR01jG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
