{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "$$HDC\\ Text\\ Baseline: European\\ Languages$$"
      ],
      "metadata": {
        "id": "vKsrTDTE60Q8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "OYJ_wnPf65w5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torch-hd\n",
        "\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from torchhd import functional, embeddings\n",
        "from torchhd.datasets import EuropeanLanguages as Languages\n",
        "import re, time, os\n",
        "\n",
        "def set_seed(seed=123):\n",
        "    import random, numpy as np\n",
        "    random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "set_seed(123)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfBZaCea64EG",
        "outputId": "bd16f8e2-8834-49d2-d4b5-5373391a02e5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/361.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.0/361.0 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DIMENSIONS     = 10_000\n",
        "MAX_INPUT_SIZE = 128\n",
        "BATCH_SIZE     = 256\n",
        "PADDING_IDX    = 0\n",
        "PRINT_EVERY    = 100\n",
        "\n",
        "ASCII_A = ord(\"a\")\n",
        "ASCII_Z = ord(\"z\")\n",
        "ASCII_SPACE = ord(\" \")\n",
        "NUM_TOKENS = (ASCII_Z - ASCII_A + 1) + 1 + 1  # letters + space + PAD slot\n",
        "\n",
        "def char2int(char: str) -> int:\n",
        "    a = ord(char)\n",
        "    if a == ASCII_SPACE:\n",
        "        return (ASCII_Z - ASCII_A + 1)\n",
        "    if ASCII_A <= a <= ASCII_Z:\n",
        "        return a - ASCII_A\n",
        "    return (ASCII_Z - ASCII_A + 1)  # map non a–z to space\n",
        "\n",
        "def transform(x: str) -> torch.Tensor:\n",
        "    x = x.lower()\n",
        "    x = re.sub(r\"\\s+\", \" \", x)\n",
        "    x = x[:MAX_INPUT_SIZE]\n",
        "    ids = [char2int(ch) + 1 for ch in x]  # shift by +1 so PAD is 0\n",
        "    if len(ids) < MAX_INPUT_SIZE:\n",
        "        ids += [PADDING_IDX] * (MAX_INPUT_SIZE - len(ids))\n",
        "    return torch.tensor(ids, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "ONgX-28K7NYn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "BtfLOo9G7ELZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = Languages(\"/content/data\", train=True,  transform=transform, download=True)\n",
        "test_ds  = Languages(\"/content/data\", train=False, transform=transform, download=True)\n",
        "\n",
        "train_ld = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=(DEVICE.type==\"cuda\"))\n",
        "test_ld  = data.DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=(DEVICE.type==\"cuda\"))\n",
        "\n",
        "len(train_ds), len(test_ds), train_ds.classes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqxS_tlA7Daz",
        "outputId": "e74e87de-2048-4953-92dc-a54ecfb1c972"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zCvjPf0R5pOR46CNBNMM60b_LwQKvltI\n",
            "To: /content/data/language-recognition/data.zip\n",
            "100%|██████████| 10.3M/10.3M [00:00<00:00, 27.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(210032,\n",
              " 21000,\n",
              " ['Bulgarian',\n",
              "  'Czech',\n",
              "  'Danish',\n",
              "  'Dutch',\n",
              "  'German',\n",
              "  'English',\n",
              "  'Estonian',\n",
              "  'Finnish',\n",
              "  'French',\n",
              "  'Greek',\n",
              "  'Hungarian',\n",
              "  'Italian',\n",
              "  'Latvian',\n",
              "  'Lithuanian',\n",
              "  'Polish',\n",
              "  'Portuguese',\n",
              "  'Romanian',\n",
              "  'Slovak',\n",
              "  'Slovenian',\n",
              "  'Spanish',\n",
              "  'Swedish'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "_FM1m199wi2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize"
      ],
      "metadata": {
        "id": "z1WkH4_nwnf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes, vocab_size, dim, padding_idx=0):\n",
        "        super().__init__()\n",
        "        self.symbol = embeddings.Random(vocab_size, dim, padding_idx=padding_idx)\n",
        "        self.classify = nn.Linear(dim, num_classes, bias=False)\n",
        "        with torch.no_grad():\n",
        "            self.classify.weight.zero_()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode(self, x_ids: torch.Tensor) -> torch.Tensor:\n",
        "        # We rely on TorchHD's ngrams (n=3) and hard_quantize, identical to the example.\n",
        "        symbols = self.symbol(x_ids)                 # [B, T, D]\n",
        "        hv = functional.ngrams(symbols, n=3)         # [B, D]\n",
        "        hv = functional.hard_quantize(hv)            # sign -> {-1,+1}\n",
        "        return hv\n",
        "\n",
        "    def forward(self, x_ids: torch.Tensor) -> torch.Tensor:\n",
        "        enc = self.encode(x_ids)                     # [B, D]\n",
        "        return self.classify(enc)                    # [B, C]\n",
        "\n",
        "model = Model(len(train_ds.classes), NUM_TOKENS, DIMENSIONS, padding_idx=PADDING_IDX).to(DEVICE)"
      ],
      "metadata": {
        "id": "qDM3OmWYwm5M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train prototypes"
      ],
      "metadata": {
        "id": "hgQP3eIWwkpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train: single-pass prototype accumulation\n",
        "t0 = time.time()\n",
        "with torch.no_grad():\n",
        "    for bi, (samples, labels) in enumerate(train_ld, 1):\n",
        "        samples = samples.to(DEVICE, non_blocking=True)\n",
        "        labels  = labels.to(DEVICE, non_blocking=True)\n",
        "        samples_hv = model.encode(samples)                          # [B, D], bipolar\n",
        "        model.classify.weight.index_add_(0, labels, samples_hv)     # accumulate into class rows\n",
        "        if bi % PRINT_EVERY == 0:\n",
        "            print(f\"[train] {bi}/{len(train_ld)}\")\n",
        "            print(f\"  |  elapsed: {time.time() - t0:.1f}s\")\n",
        "\n",
        "    # Normalize class rows (cosine-like scoring)\n",
        "    model.classify.weight[:] = F.normalize(model.classify.weight, dim=1)\n",
        "    print(f\"Total Time Elapsed: {time.time() - t0:.1f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89GTfYBXwjhu",
        "outputId": "c8759ae0-52d9-4776-ed00-ef4ae30927ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1691475981.py:14: DeprecationWarning: torchhd.hard_quantize is deprecated, consider using torchhd.normalize instead.\n",
            "  hv = functional.hard_quantize(hv)            # sign -> {-1,+1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 100/821\n",
            "  |  elapsed: 11.1s\n",
            "[train] 200/821\n",
            "  |  elapsed: 22.1s\n",
            "[train] 300/821\n",
            "  |  elapsed: 33.1s\n",
            "[train] 400/821\n",
            "  |  elapsed: 44.1s\n",
            "[train] 500/821\n",
            "  |  elapsed: 55.0s\n",
            "[train] 600/821\n",
            "  |  elapsed: 66.0s\n",
            "[train] 700/821\n",
            "  |  elapsed: 77.0s\n",
            "[train] 800/821\n",
            "  |  elapsed: 87.9s\n",
            "Total Time Elapsed: 90.3s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prototypes = model.classify.weight\n",
        "print(\"Prototypes shape:\", prototypes.shape)\n",
        "print(\"First prototype (for the first class):\", prototypes[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nvpHdkHARYM",
        "outputId": "356e21d0-4261-49d1-8024-75ce5815c91d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prototypes shape: torch.Size([21, 10000])\n",
            "First prototype (for the first class): tensor([ 0.0043, -0.0015, -0.0012,  ..., -0.0026,  0.0051,  0.0028],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(prototypes, \"prototypes_EULanguage.pt\")"
      ],
      "metadata": {
        "id": "aOoCn40bxLIu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "c_Gc3C6iw4kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.load(\"prototypes_EULanguage.pt\")"
      ],
      "metadata": {
        "id": "tfaFfHFKxVsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "correct = total = 0\n",
        "with torch.no_grad():\n",
        "    t1 = time.time()\n",
        "    for bi, (samples, labels) in enumerate(test_ld, 1):\n",
        "        samples = samples.to(DEVICE, non_blocking=True)\n",
        "        labels  = labels.to(DEVICE, non_blocking=True)\n",
        "        outputs = model(samples)\n",
        "        preds   = outputs.argmax(dim=-1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total   += labels.numel()\n",
        "        if bi % PRINT_EVERY == 0:\n",
        "            print(f\"[test ] {bi}/{len(test_ld)}\")\n",
        "acc = correct / total\n",
        "print(f\"Test accuracy (EU languages): {acc:.4f}  |  elapsed: {time.time() - t0:.1f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IcdJx-MwvJF",
        "outputId": "a21cbca3-5d40-469e-8677-7fcb086c07a6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1691475981.py:14: DeprecationWarning: torchhd.hard_quantize is deprecated, consider using torchhd.normalize instead.\n",
            "  hv = functional.hard_quantize(hv)            # sign -> {-1,+1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy (EU languages): 0.9733  |  elapsed: 250.6s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zGqZCqDR01jG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}